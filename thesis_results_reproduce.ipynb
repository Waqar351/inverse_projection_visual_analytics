{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "data = iris['data']\n",
    "target = iris['target']\n",
    "target_names = iris['target_names']\n",
    "feature_names = iris['feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Standardize the dataset\n",
    "# scaler = StandardScaler()\n",
    "# data_standardized = scaler.fit_transform(data)\n",
    "\n",
    "# # Compute the covariance matrix of the standardized data\n",
    "# covariance_matrix = np.cov(data_standardized.T)\n",
    "\n",
    "# # Print the covariance matrix\n",
    "# print(\"Covariance Matrix of Standardized Data:\")\n",
    "# print(covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center the data (subtract the mean of each feature)\n",
    "data_centered = data - np.mean(data, axis=0)\n",
    "\n",
    "# Compute the covariance matrix of the centered data\n",
    "covariance_matrix = np.cov(data_centered.T)\n",
    "\n",
    "# Print the covariance matrix\n",
    "print(\"Covariance Matrix of Centered Data:\")\n",
    "print(covariance_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA\n",
    "pca = PCA(n_components=2)  # Reduce to 2 components for visualization\n",
    "pca_result = pca.fit_transform(data_centered)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df = pd.DataFrame(pca_result, columns=['PC1', 'PC2'])\n",
    "df['Target'] = target\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA results\n",
    "%matplotlib qt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, target_name in enumerate(target_names):\n",
    "    plt.scatter(\n",
    "        df[df['Target'] == i]['PC1'],\n",
    "        df[df['Target'] == i]['PC2'],\n",
    "        color=colors[i],\n",
    "        label=target_name,\n",
    "        alpha=0.7,\n",
    "        edgecolors='k'\n",
    "    )\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"PCA of Iris Dataset\", fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# Sort eigenvalues and eigenvectors in descending order\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]\n",
    "eigenvalues = eigenvalues[sorted_indices]\n",
    "eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Compute the principal components\n",
    "principal_components = np.dot(data_centered, eigenvectors[:, :2])\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df = pd.DataFrame(principal_components, columns=['PC1', 'PC2'])\n",
    "df['Target'] = target\n",
    "\n",
    "# Plot PCA results\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, target_name in enumerate(target_names):\n",
    "    plt.scatter(\n",
    "        df[df['Target'] == i]['PC1'],\n",
    "        df[df['Target'] == i]['PC2'],\n",
    "        color=colors[i],\n",
    "        label=target_name,\n",
    "        alpha=0.7,\n",
    "        edgecolors='k'\n",
    "    )\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"PCA of Iris Dataset (Using Eigenvalues and Eigenvectors)\", fontsize=16)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply t-SNE with perplexity 15\n",
    "tsne = TSNE(n_components=2, perplexity=15, random_state=42)\n",
    "tsne_results = tsne.fit_transform(data_centered)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df_tsne = pd.DataFrame(tsne_results, columns=['Dim1', 'Dim2'])\n",
    "df_tsne['Target'] = target\n",
    "\n",
    "# Plot t-SNE results\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, target_name in enumerate(target_names):\n",
    "    plt.scatter(\n",
    "        df_tsne[df_tsne['Target'] == i]['Dim1'],\n",
    "        df_tsne[df_tsne['Target'] == i]['Dim2'],\n",
    "        color=colors[i],\n",
    "        label=target_name,\n",
    "        alpha=0.7,\n",
    "        edgecolors='k'\n",
    "    )\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"t-SNE of Iris Dataset (Perplexity = 15)\", fontsize=16)\n",
    "# plt.xlabel(\"t-SNE Dimension 1\", fontsize=12)\n",
    "# plt.ylabel(\"t-SNE Dimension 2\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply UMAP\n",
    "umap_reducer = umap.UMAP(n_neighbors=15, \n",
    "                         min_dist=0.1, \n",
    "                         n_components=2, \n",
    "                         random_state=42)\n",
    "umap_results = umap_reducer.fit_transform(data_centered)\n",
    "# umap_results = umap_reducer.fit_transform(data)\n",
    "\n",
    "# Create a DataFrame for visualization\n",
    "df_umap = pd.DataFrame(umap_results, columns=['UMAP1', 'UMAP2'])\n",
    "df_umap['Target'] = target\n",
    "\n",
    "# Plot UMAP results\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red', 'green', 'blue']\n",
    "for i, target_name in enumerate(target_names):\n",
    "    plt.scatter(\n",
    "        df_umap[df_umap['Target'] == i]['UMAP1'],\n",
    "        df_umap[df_umap['Target'] == i]['UMAP2'],\n",
    "\n",
    "        \n",
    "        color=colors[i],\n",
    "        label=target_name,\n",
    "        alpha=0.7,\n",
    "        edgecolors='k'\n",
    "    )\n",
    "\n",
    "# Add labels and title\n",
    "plt.title(\"UMAP of Iris Dataset\", fontsize=16)\n",
    "plt.xlabel(\"UMAP Dimension 1\", fontsize=12)\n",
    "plt.ylabel(\"UMAP Dimension 2\", fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star glyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=data, columns=feature_names)\n",
    "iris_df['species'] = iris.target\n",
    "iris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "# Filter data for Virginica class\n",
    "virginica_data = iris_df[iris_df['species'] == 'setosa']\n",
    "\n",
    "# Define features to plot (Iris features are 4)\n",
    "# features = iris.feature_names\n",
    "\n",
    "# Create the star glyph visualization\n",
    "def plot_star_glyphs(data, features, ax):\n",
    "    # Select only the numeric columns (features)\n",
    "    data_numeric = data[features]\n",
    "    \n",
    "    # Normalize data to make sure all rays have the same length\n",
    "    data_norm = data_numeric / np.max(data_numeric)\n",
    "    \n",
    "    angles = np.linspace(0, 2 * np.pi, len(features), endpoint=False)\n",
    "    \n",
    "    # Plot each data point as a star glyph\n",
    "    for i, row in data_numeric.iterrows():\n",
    "        values = row.values\n",
    "        values_norm = values / np.max(values)  # Normalize the values\n",
    "        \n",
    "        ax.plot(np.append(angles, angles[0]), np.append(values_norm, values_norm[0]), label=f'Point {i}', marker='o')\n",
    "        ax.fill(np.append(angles, angles[0]), np.append(values_norm, values_norm[0]), alpha=0.2)\n",
    "\n",
    "# Create a polar plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw={'projection': 'polar'})\n",
    "\n",
    "# Plot the Virginica data as star glyphs\n",
    "plot_star_glyphs(virginica_data, feature_names, ax)\n",
    "\n",
    "ax.set_xticks(np.linspace(0, 2 * np.pi, len(feature_names), endpoint=False))\n",
    "ax.set_xticklabels(feature_names)\n",
    "ax.set_title('Star Glyphs Representation for Virginica Class (Iris Dataset)', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sepal_length_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "feature_names = iris.feature_names\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create a DataFrame for the dataset\n",
    "iris_df = pd.DataFrame(data=data, columns=feature_names)\n",
    "iris_df['species'] = target\n",
    "iris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "# Filter data for Virginica class\n",
    "virginica_data = iris_df[iris_df['species'] == 'virginica']\n",
    "\n",
    "# Select the observation with the maximum sepal length\n",
    "max_sepal_length_data = virginica_data.loc[virginica_data['sepal length (cm)'].idxmax()]\n",
    "\n",
    "# Define the desired feature order\n",
    "ordered_features = ['sepal length (cm)', 'petal width (cm)', 'petal length (cm)', 'sepal width (cm)']\n",
    "\n",
    "# Create the star glyph visualization for one observation\n",
    "def plot_single_star_glyph(data_row, features, ax):\n",
    "    # Reorder the values according to the specified feature order\n",
    "    values = data_row[features].values\n",
    "\n",
    "    # Normalize the values to make sure all rays have the same length\n",
    "    values_norm = values / np.max(values)\n",
    "\n",
    "    # Angles for each feature, arranged in the specified order\n",
    "    angles = np.linspace(0, 2 * np.pi, len(features), endpoint=False)\n",
    "    angles = np.append(angles, angles[0])  # Close the polygon\n",
    "\n",
    "    # Values for plotting\n",
    "    values_norm = np.append(values_norm, values_norm[0])  # Close the polygon\n",
    "\n",
    "    # Plot the star glyph\n",
    "    ax.plot(angles, values_norm, label='Max Sepal Length', marker='o')\n",
    "    ax.fill(angles, values_norm, alpha=0.2)\n",
    "\n",
    "    # Annotate each corner with the original value\n",
    "    for angle, value, norm_value in zip(angles, values, values_norm[:-1]):  # Skip the duplicate closing point\n",
    "        ax.text(\n",
    "            angle, norm_value - 0.1,  # Adjust text position slightly outward\n",
    "            f\"{value:.2f}\", \n",
    "            ha='center', va='center', fontsize=10, color='blue'\n",
    "        )\n",
    "\n",
    "# Create a polar plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw={'projection': 'polar'})\n",
    "\n",
    "# Plot the single observation with the maximum sepal length\n",
    "plot_single_star_glyph(max_sepal_length_data, ordered_features, ax)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xticks(np.linspace(0, 2 * np.pi, len(ordered_features), endpoint=False))\n",
    "ax.set_xticklabels(ordered_features)\n",
    "ax.set_title('Star Glyph Representation for Virginica Observation with Max Sepal Length', fontsize=14)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "feature_names = iris.feature_names\n",
    "data = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# Create a DataFrame for the dataset\n",
    "iris_df = pd.DataFrame(data=data, columns=feature_names)\n",
    "iris_df['species'] = target\n",
    "iris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "# Filter data for Virginica class\n",
    "virginica_data = iris_df[iris_df['species'] == 'setosa']\n",
    "\n",
    "# Select the observation with the maximum sepal length\n",
    "max_sepal_length_data = virginica_data.loc[virginica_data['sepal length (cm)'].idxmax()]\n",
    "\n",
    "# Define the desired feature order\n",
    "ordered_features = ['sepal length (cm)', 'petal width (cm)', 'petal length (cm)', 'sepal width (cm)']\n",
    "\n",
    "# Create the star glyph visualization for one observation\n",
    "def plot_single_star_glyph(data_row, features, ax):\n",
    "    # Reorder the values according to the specified feature order\n",
    "    values = data_row[features].values\n",
    "\n",
    "    # Normalize the values to make sure all rays have the same length\n",
    "    values_norm = values / np.max(values)\n",
    "\n",
    "    # Angles for each feature, arranged in the specified order\n",
    "    angles = np.linspace(0, 2 * np.pi, len(features), endpoint=False)\n",
    "    angles = np.append(angles, angles[0])  # Close the polygon\n",
    "\n",
    "    # Values for plotting\n",
    "    values_norm = np.append(values_norm, values_norm[0])  # Close the polygon\n",
    "\n",
    "    # Plot the star glyph\n",
    "    ax.plot(angles, values_norm, label='Max Sepal Length', marker='o')\n",
    "    ax.fill(angles, values_norm, alpha=0.2)\n",
    "\n",
    "    # Annotate each corner with the original value\n",
    "    for angle, value, norm_value in zip(angles, values, values_norm[:-1]):  # Skip the duplicate closing point\n",
    "        ax.text(\n",
    "            angle, norm_value + 0.05,  # Adjust text position slightly outward\n",
    "            f\"{value:.2f}\", \n",
    "            ha='center', va='center', fontsize=10, color='blue'\n",
    "        )\n",
    "\n",
    "# Create a polar plot\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw={'projection': 'polar'})\n",
    "\n",
    "# Plot the single observation with the maximum sepal length\n",
    "plot_single_star_glyph(max_sepal_length_data, ordered_features, ax)\n",
    "\n",
    "# Customize the plot\n",
    "ax.set_xticks(np.linspace(0, 2 * np.pi, len(ordered_features), endpoint=False))\n",
    "ax.set_xticklabels(ordered_features)\n",
    "ax.set_title('Star Glyph Representation for Setosa Observation with Max Sepal Length', fontsize=14)\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UMPA & Star Glyphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import umap\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['species'] = iris.target\n",
    "iris_df['species'] = iris_df['species'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})\n",
    "\n",
    "# Perform UMAP dimensionality reduction\n",
    "umap_model = umap.UMAP(n_components=2, \n",
    "                       min_dist=0.1,\n",
    "                       n_neighbors= 15,\n",
    "                       random_state=42)\n",
    "\n",
    "# umap_result = umap_model.fit_transform(iris.data)\n",
    "umap_result = umap_model.fit_transform(data_centered)\n",
    "\n",
    "# Create a DataFrame for UMAP results\n",
    "umap_df = pd.DataFrame(umap_result, columns=['UMAP1', 'UMAP2'])\n",
    "umap_df['species'] = iris_df['species']\n",
    "\n",
    "# Map species labels to integers for color indexing\n",
    "species_map = {'setosa': 0, 'versicolor': 1, 'virginica': 2}\n",
    "umap_df['species_int'] = umap_df['species'].map(species_map)\n",
    "\n",
    "# Function to plot polygon glyphs on the UMAP plot\n",
    "def plot_umap_polygon_glyphs(data, features, ax, size_factor=0.5):\n",
    "    num_features = len(features)\n",
    "    angles = np.linspace(0, 2 * np.pi, num_features, endpoint=False)\n",
    "\n",
    "    # Plot each data point as a polygon glyph\n",
    "    for i, row in data.iterrows():\n",
    "        # Get the feature values for the current data point from the original Iris dataset\n",
    "        values = iris_df.loc[i, features].values  # Access features from the original dataset\n",
    "        values_norm = values / np.max(values)  # Normalize the values to range [0, 1]\n",
    "\n",
    "        # Create the polygon by connecting the normalized values to their corresponding angles\n",
    "        polygon = np.append(values_norm, values_norm[0])\n",
    "        polygon_angles = np.append(angles, angles[0])\n",
    "        \n",
    "        # Apply the size_factor to scale down the polygon size\n",
    "        polygon *= size_factor  # Scale the polygon size by the factor\n",
    "        \n",
    "        # Map UMAP coordinates to the center of the polygon\n",
    "        x, y = row['UMAP1'], row['UMAP2']\n",
    "        \n",
    "        # Create a polygon around the UMAP coordinates (using scatter)\n",
    "        polygon_x = x + np.cos(polygon_angles) * polygon\n",
    "        polygon_y = y + np.sin(polygon_angles) * polygon\n",
    "        \n",
    "        # Plot the polygon\n",
    "        ax.fill(polygon_x, polygon_y, alpha=0.7, color=sns.color_palette(\"Set2\")[row['species_int']])\n",
    "        ax.plot(polygon_x, polygon_y, alpha=0.7, color='black')\n",
    "\n",
    "    # Set the UMAP plot limits\n",
    "    ax.set_xlim(data['UMAP1'].min() - 1, data['UMAP1'].max() + 1)\n",
    "    ax.set_ylim(data['UMAP2'].min() - 1, data['UMAP2'].max() + 1)\n",
    "\n",
    "# Create the UMAP plot with polygon glyphs\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot UMAP results using polygon glyphs with a reduced size (adjust the size_factor as needed)\n",
    "plot_umap_polygon_glyphs(umap_df, iris.feature_names, ax, size_factor=0.3)  # Size factor can be adjusted\n",
    "\n",
    "# Customize the UMAP plot with species labels\n",
    "sns.scatterplot(x='UMAP1', y='UMAP2', hue='species', data=umap_df, palette='Set2', ax=ax, legend='full')\n",
    "\n",
    "ax.set_title('UMAP Visualization of Iris Dataset with Smaller Polygon Glyphs', fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define min and max values\n",
    "x_min, x_max = np.min(umap_result[:, 0]), np.max(umap_result[:, 0])\n",
    "y_min, y_max = np.min(umap_result[:, 1]), np.max(umap_result[:, 1])\n",
    "print(x_min, x_max)\n",
    "print(y_min, y_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid resolution\n",
    "num_grid_points = 100\n",
    "\n",
    "# Generate grid\n",
    "x_vals = np.linspace(x_min, x_max, num_grid_points)\n",
    "y_vals = np.linspace(y_min, y_max, num_grid_points)\n",
    "xx, yy = np.meshgrid(x_vals, y_vals)\n",
    "print(yy.shape)\n",
    "print(y_vals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "# Visualize the grid on top of the t-SNE data\n",
    "plt.scatter(umap_result[:, 0], umap_result[:, 1], c='blue', s=10, label=\"UMAP Output\")\n",
    "plt.scatter(xx, yy, c='red', s=5, label=\"Grid Points\")\n",
    "plt.title(\"2D t-SNE Output with Grid Points\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "# plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the MLP inverse_model\n",
    "class NNinv(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(NNinv, self).__init__()\n",
    "        \n",
    "        # Define the layers\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),  # Input to first hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),  # First hidden layer to second hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),  # Second hidden layer to third hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),  # Third hidden layer to fourth hidden layer\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_size),  # Fifth hidden layer to output\n",
    "            nn.Sigmoid()  # Output layer with sigmoid activation\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(umap_result, iris.data, test_size=0.33, random_state=42, stratify=iris.target)\n",
    "X_train, X_test, y_train, y_test, c_train, c_test = train_test_split(umap_result, data_centered,iris.target, test_size=0.33, random_state=42, stratify=iris.target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_size = 2  # Example input size (can be changed)\n",
    "output_size = 4   # Binary classification (sigmoid output for single output)\n",
    "\n",
    "# Create DataLoader for batch processing\n",
    "batch_size = 64\n",
    "t_X_train = torch.tensor(X_train)\n",
    "t_y_train = torch.tensor(y_train)\n",
    "dataset = TensorDataset(t_X_train, t_y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Instantiate the inverse_model, loss function, and optimizer\n",
    "inverse_model = NNinv(input_size, output_size)\n",
    "loss_fn = nn.L1Loss()  # Mean Absolute Error (MAE)\n",
    "optimizer = optim.Adam(inverse_model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs to train\n",
    "num_epochs = 500\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(dataloader):\n",
    "        # Forward pass\n",
    "        outputs = inverse_model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print the average loss for the epoch\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    # print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "t_X_test = torch.tensor(X_test)\n",
    "t_y_test = torch.tensor(y_test)\n",
    "outputs_test = inverse_model(t_X_test)\n",
    "loss_test = loss_fn(outputs_test, t_y_test)\n",
    "print(loss_test/y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define grid resolution \n",
    "num_grid_points = 100\n",
    "\n",
    "# Generate grid\n",
    "x_vals = np.linspace(x_min, x_max, num_grid_points)\n",
    "y_vals = np.linspace(y_min, y_max, num_grid_points)\n",
    "xx, yy = np.meshgrid(x_vals, y_vals)\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "\n",
    "\n",
    "jacobian_norms = np.zeros(len(grid_points))\n",
    "for idx, point in enumerate(grid_points):\n",
    "    point_tensor = torch.tensor(point, dtype=torch.float32, requires_grad=True).view(1, 2)\n",
    "    \n",
    "    # Compute the Jacobian for the current point\n",
    "    jacobian = torch.autograd.functional.jacobian(lambda x: inverse_model(x), point_tensor)\n",
    "    \n",
    "    # Reshape Jacobian to 2D: (output_dim, input_dim)\n",
    "    jacobian_2d = jacobian.view(4, 2)  # Assuming output is (1, 3), input is (1, 2)\n",
    "    \n",
    "    # Compute spectral norm (largest singular value)\n",
    "    jacobian_norms[idx] = torch.linalg.norm(jacobian_2d, ord=2).item()\n",
    "\n",
    "jacobian_norms = jacobian_norms.reshape(xx.shape)\n",
    "\n",
    "# Step 4: Plot heatmap with t-SNE points overlayed\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Overlay t-SNE points\n",
    "# plt.scatter(S[:, 0], S[:, 1], c='blue', edgecolor='k', label='t-SNE points')\n",
    "c=iris.target\n",
    "for i in range(3):\n",
    "    # plt.scatter(umap_result[c == i, 0], umap_result[c == i, 1], color=colors[i], label=f'Gaussian{i+1}', edgecolor=None)\n",
    "    plt.scatter(umap_result[c == i, 0], umap_result[c == i, 1], label=f'Gaussian{i+1}', edgecolor=None)\n",
    "\n",
    "# Plot heatmap\n",
    "plt.imshow(\n",
    "    jacobian_norms,\n",
    "    extent=(x_min, x_max, y_min, y_max),\n",
    "    origin='lower',\n",
    "    cmap='hot',\n",
    "    alpha=1\n",
    ")\n",
    "plt.colorbar(label='Spectral Norm of Jacobian')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Labels and title\n",
    "plt.title(\"Overlaying UMAP points on Jacobian Heatmap\")\n",
    "plt.xlabel(\"UMAP Dimension 1\")\n",
    "plt.ylabel(\"UMAP Dimension 2\")\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decisioin Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from umap import UMAP\n",
    "\n",
    "# Loading the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "feature_names = iris.feature_names\n",
    "target_names = iris.target_names\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Scaling the features (important for SVC)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Training the SVC model with RBF kernel\n",
    "svc_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "svc_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Reducing the dimensionality of the data with UMAP\n",
    "umap_model = UMAP(n_components=2, random_state=42)\n",
    "X_umap = umap_model.fit_transform(X_train_scaled)\n",
    "\n",
    "# Generating a grid of points for visualization\n",
    "x_min, x_max = X_umap[:, 0].min() - 1, X_umap[:, 0].max() + 1\n",
    "y_min, y_max = X_umap[:, 1].min() - 1, X_umap[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 100), np.linspace(y_min, y_max, 100))\n",
    "grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "\n",
    "# Predicting the class and probabilities for each grid point\n",
    "grid_points_scaled = scaler.transform(umap_model.inverse_transform(grid_points))\n",
    "probs = svc_model.predict_proba(grid_points_scaled)\n",
    "\n",
    "# Reshaping the predicted probabilities\n",
    "probs_reshaped = probs[:, 1].reshape(xx.shape)\n",
    "\n",
    "# Plotting the decision boundary\n",
    "fig = go.Figure()\n",
    "\n",
    "# Adding decision boundary trace\n",
    "fig.add_trace(go.Contour(\n",
    "    x=xx[0],\n",
    "    y=yy[:, 0],\n",
    "    z=probs_reshaped,\n",
    "    colorscale='Viridis',  # You can change this to any other colorscale\n",
    "    opacity=0.6,\n",
    "    showscale=True,\n",
    "    line_smoothing=0.8,\n",
    "    colorbar=dict(title=\"Probability\")\n",
    "))\n",
    "\n",
    "# Adding training points trace\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_umap[:, 0],\n",
    "    y=X_umap[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        color=y_train,\n",
    "        colorscale='Viridis',\n",
    "        size=10,\n",
    "        colorbar=dict(title=\"Species\")\n",
    "    ),\n",
    "    text=target_names[y_train],\n",
    "    hoverinfo='text'\n",
    "))\n",
    "\n",
    "# Layout settings\n",
    "fig.update_layout(\n",
    "    title=\"Decision Boundary Map with SVC and UMAP\",\n",
    "    xaxis_title='UMAP Component 1',\n",
    "    yaxis_title='UMAP Component 2',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.linalg import svd\n",
    "\n",
    "# Step 1: Load Iris dataset and preprocess it\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Standardize the data (important for UMAP)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 2: Apply UMAP to the data\n",
    "umap_model = umap.UMAP(n_neighbors=15, min_dist=0.1, metric='euclidean')\n",
    "X_umap = umap_model.fit_transform(X_scaled)\n",
    "\n",
    "# Step 3: Create a grid of points in the 2D UMAP space\n",
    "x_min, x_max = X_umap[:, 0].min() - 1, X_umap[:, 0].max() + 1\n",
    "y_min, y_max = X_umap[:, 1].min() - 1, X_umap[:, 1].max() + 1\n",
    "\n",
    "# Define the resolution of the grid\n",
    "grid_resolution = 50\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, grid_resolution), np.linspace(y_min, y_max, grid_resolution))\n",
    "\n",
    "# Step 4: Define a function to estimate the Jacobian matrix over the grid using inverse UMAP\n",
    "def jacobian_spectral_norm_grid_inverse(X_grid, model, epsilon=1e-5):\n",
    "    \"\"\"Compute the spectral norm of the Jacobian for a grid of points using inverse UMAP.\"\"\"\n",
    "    spectral_norms = np.zeros(X_grid.shape[0])\n",
    "    \n",
    "    for i, point in enumerate(X_grid):\n",
    "        # Perturb the point in both dimensions\n",
    "        jacobian_matrix = np.zeros((X.shape[1], 2))  # Jacobian is high-dimensional x 2\n",
    "        \n",
    "        for j in range(2):  # Two dimensions for UMAP output\n",
    "            perturbed_point = point.copy()\n",
    "            perturbed_point[j] += epsilon  # Perturb along one axis\n",
    "            \n",
    "            # Get the transformed (perturbed) points in the 2D UMAP space\n",
    "            perturbed_projection = model.transform([perturbed_point])\n",
    "            original_projection = model.transform([point])\n",
    "            \n",
    "            # Apply inverse UMAP to get back to original high-dimensional space\n",
    "            perturbed_inverse = model.inverse_transform(perturbed_projection)\n",
    "            original_inverse = model.inverse_transform(original_projection)\n",
    "            \n",
    "            # Estimate the Jacobian column by column\n",
    "            jacobian_matrix[:, j] = (perturbed_inverse - original_inverse) / epsilon\n",
    "        \n",
    "        # Compute the spectral norm (largest singular value) of the Jacobian matrix\n",
    "        _, s, _ = svd(jacobian_matrix)\n",
    "        spectral_norms[i] = s[0]\n",
    "    \n",
    "    return spectral_norms\n",
    "\n",
    "# Step 5: Flatten the grid and compute the Jacobian's spectral norm for each grid point\n",
    "grid_points = np.column_stack([xx.ravel(), yy.ravel()])\n",
    "spectral_norms_grid = jacobian_spectral_norm_grid_inverse(grid_points, umap_model)\n",
    "\n",
    "# Step 6: Reshape the spectral norms to match the grid shape for visualization\n",
    "spectral_norms_grid = spectral_norms_grid.reshape(xx.shape)\n",
    "\n",
    "# Step 7: Visualize the spectral norm heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.contourf(xx, yy, spectral_norms_grid, 20, cmap='viridis')\n",
    "plt.colorbar(label='Spectral Norm of Jacobian')\n",
    "plt.title('Spectral Norm Heatmap of Jacobian (Inverse UMAP) for Iris Dataset')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.scatter(X_umap[:, 0], X_umap[:, 1], c='red', s=10, label='Data Points')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.manifold import trustworthiness\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "data = iris['data']\n",
    "c = iris['target']\n",
    "\n",
    "# Center the data (subtract the mean of each feature)\n",
    "D = data - np.mean(data, axis=0)\n",
    "\n",
    "# Prepare to calculate trustworthiness for different perplexities\n",
    "perplexities = [2, 3, 4, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 80, 90, 100]\n",
    "trustworthiness_values = []\n",
    "\n",
    "method = \"tsne\"\n",
    "\n",
    "for perplexity in perplexities:\n",
    "    if method == \"tsne\":\n",
    "        reducer = manifold.TSNE(n_components=2, perplexity=perplexity, init=\"random\", random_state=0)\n",
    "        method_name = \"t-SNE\"\n",
    "    elif method == \"umap\":\n",
    "        reducer = umap.UMAP(n_components=2, n_neighbors=perplexity, min_dist=0.1, init=\"random\", random_state=0)\n",
    "        method_name = \"UMAP\"\n",
    "    elif method == \"pca\":\n",
    "        reducer = PCA(n_components=2, random_state=0)\n",
    "        method_name = \"PCA\"\n",
    "    \n",
    "    S = reducer.fit_transform(D)\n",
    "\n",
    "    # Calculate trustworthiness between the original high-dimensional data and the reduced 2D data\n",
    "    trust = manifold.trustworthiness(D, S, n_neighbors=70)\n",
    "    trustworthiness_values.append(trust)\n",
    "\n",
    "# Plot trustworthiness for different perplexities\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(perplexities, trustworthiness_values, marker='o')\n",
    "plt.title(f\"Trustworthiness vs Perplexity for {method_name}\")\n",
    "plt.xlabel(\"Perplexity\")\n",
    "plt.ylabel(\"Trustworthiness\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.linalg import norm\n",
    "\n",
    "# Step 1: Load and preprocess the Iris dataset\n",
    "data = load_iris()\n",
    "features = data['data']\n",
    "labels = data['target']\n",
    "n_samples = features.shape[0]\n",
    "n_features = features.shape[1]\n",
    "\n",
    "# Normalize the features to [0, 1] range\n",
    "scaler = MinMaxScaler()\n",
    "norm_features = scaler.fit_transform(features)\n",
    "\n",
    "# Step 2: Define a grid for visualization\n",
    "# Assigning each sample to a grid point (e.g., 10x10 grid for 100 samples)\n",
    "grid_size = int(np.ceil(np.sqrt(n_samples)))\n",
    "grid = np.zeros((grid_size, grid_size, n_features))\n",
    "grid_labels = np.zeros((grid_size, grid_size))\n",
    "\n",
    "# Fill the grid with feature vectors and labels\n",
    "for idx, feature in enumerate(norm_features):\n",
    "    i, j = divmod(idx, grid_size)\n",
    "    grid[i, j, :] = feature\n",
    "    grid_labels[i, j] = labels[idx]\n",
    "\n",
    "# Step 3: Compute the Jacobian matrix for each grid point\n",
    "jacobian_norms = np.zeros((grid_size, grid_size))\n",
    "\n",
    "def compute_jacobian(grid, i, j):\n",
    "    \"\"\"Compute the approximate Jacobian matrix for grid point (i, j).\"\"\"\n",
    "    neighbors = []\n",
    "    if i > 0:  # Top neighbor\n",
    "        neighbors.append(grid[i - 1, j])\n",
    "    if i < grid_size - 1:  # Bottom neighbor\n",
    "        neighbors.append(grid[i + 1, j])\n",
    "    if j > 0:  # Left neighbor\n",
    "        neighbors.append(grid[i, j - 1])\n",
    "    if j < grid_size - 1:  # Right neighbor\n",
    "        neighbors.append(grid[i, j + 1])\n",
    "\n",
    "    jacobian = []\n",
    "    for neighbor in neighbors:\n",
    "        jacobian.append(neighbor - grid[i, j])\n",
    "\n",
    "    return np.array(jacobian).T\n",
    "\n",
    "# Calculate the spectral norm of the Jacobian for each grid point\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        jacobian = compute_jacobian(grid, i, j)\n",
    "        spectral_norm = norm(jacobian, ord=2)  # Spectral norm of the Jacobian\n",
    "        jacobian_norms[i, j] = spectral_norm\n",
    "\n",
    "# Step 4: Visualize the spectral norm of the Jacobian as a heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.imshow(jacobian_norms, cmap='viridis', interpolation='nearest')\n",
    "plt.colorbar(label='Spectral Norm of Jacobian')\n",
    "plt.title('Heatmap of Spectral Norm of Jacobian (Iris Dataset)')\n",
    "plt.xlabel('Grid X-axis')\n",
    "plt.ylabel('Grid Y-axis')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trustworthiness & Continuity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import manifold\n",
    "import umap\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.manifold import trustworthiness\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "data = iris['data']\n",
    "c = iris['target']\n",
    "\n",
    "# Center the data (subtract the mean of each feature)\n",
    "D = data - np.mean(data, axis=0)\n",
    "\n",
    "method = \"tsne\"\n",
    "\n",
    "perplexity = 15\n",
    "reducer = manifold.TSNE(n_components=2, perplexity=perplexity, init=\"random\", random_state=0)\n",
    "\n",
    "n_neighbors=7\n",
    "metric=\"euclidean\"\n",
    "X =D\n",
    "X_emb = reducer.fit_transform(D)\n",
    "\n",
    "# Calculate trustworthiness between the original high-dimensional data and the reduced 2D data\n",
    "trust = manifold.trustworthiness(X, X_emb, n_neighbors=n_neighbors)\n",
    "\n",
    "trust\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zadu_measure.trustworthiness_continuity import measure_tnc\n",
    "from zadu_measure.neighborhood_hit import measure_nh\n",
    "from zadu_measure.mean_relative_rank_error import measure_mrre\n",
    "from zadu_measure.spearman_rho import measure_srcc\n",
    "from zadu_measure.pearson_r import measure_pcc\n",
    "from zadu_measure.distance_to_measure import measure_dtm\n",
    "from zadu_measure.class_aware_trustworthiness_continuity import measure_catnc\n",
    "from zadu_measure.clustering_and_external_validation_measure import measure_cevmv\n",
    "from zadu_measure.distance_consistency import measure_dsc\n",
    "from zadu_measure.internal_validation_measure import measure_ivmv\n",
    "from zadu_measure.kl_divergence import measure_kl\n",
    "from zadu_measure.label_trustworthiness_and_continuity import measure_ltnc\n",
    "from zadu_measure.local_continuity_meta_criteria import measure_lcmc\n",
    "from zadu_measure.neighbor_dissimilarity import measure_nd\n",
    "from zadu_measure.non_metric_stress import measure_nms\n",
    "from zadu_measure.procrustes import measure_procrustes\n",
    "from zadu_measure.stress import measure_stress\n",
    "from zadu_measure.scale_normalized_stress import measure_norm_stress\n",
    "from zadu_measure.topographic_product import measure_topographic\n",
    "from zadu_measure.projection_precision_score import projection_precision_score_v1\n",
    "from zadu_measure.neighborhood_preservation_precision import projection_precision_score_common_neig\n",
    "from zadu_measure.average_local_error import average_local_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_tnc = measure_tnc(X, X_emb, k=7, knn_ranking_info=None, return_local=False)\n",
    "score_nh = measure_nh(X_emb, c, k=7, knn_emb_info=None, return_local=False)\n",
    "score_mrre = measure_mrre(X, X_emb, k=7, knn_ranking_info=None, return_local=False)\n",
    "score_srcc = measure_srcc(X, X_emb, distance_matrices=None)\n",
    "score_spcc = measure_pcc(X, X_emb, distance_matrices=None)\n",
    "score_dtm = measure_dtm(X, X_emb, sigma=0.1, distance_matrices=None)\n",
    "score_catnc = measure_catnc(X, X_emb, c, k=7, knn_ranking_info=None, return_local=False)\n",
    "score_cevmv = measure_cevmv(X_emb, c, measure=\"arand\",  clustering=\"kmeans\", clustering_args=None)\n",
    "score_dsc = measure_dsc(X_emb, c)\n",
    "score_ivmv = measure_ivmv(X_emb, c, measure=\"silhouette\")\n",
    "score_kl = measure_kl(X, X_emb, sigma=0.1, distance_matrices=None)\n",
    "score_ltnc = measure_ltnc(X, X_emb, c, cvm=\"dsc\")\n",
    "score_lcmc = measure_lcmc(X, X_emb, k=7, knn_info=None, return_local=False)\n",
    "score_nd = measure_nd(X, X_emb, k=7, snn_info=None, knn_info=None)\n",
    "score_nms = measure_nms(X, X_emb,distance_matrices=None)\n",
    "score_procrustes =  measure_procrustes(X, X_emb, k=7, knn_info=None)\n",
    "score_stress =  measure_stress(X, X_emb, distance_matrices=None)\n",
    "score_norm_stress =  measure_norm_stress(X, X_emb, distance_matrices=None) \n",
    "score_topographic =  measure_topographic(X, X_emb, k=7, distance_matrices=None, knn_info=None) \n",
    "score_pps =  projection_precision_score_v1(X, X_emb, n_neighbors=7)    #close to 0 is good\n",
    "score_pps_v2 =  projection_precision_score_common_neig(X, X_emb, n_neighbors=7)  # close to 1 is good\n",
    "score_ale =  average_local_error(X, X_emb)  # close to 0 is good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(score_tnc['trustworthiness'])\n",
    "print(score_tnc['continuity'])\n",
    "print(score_nh['neighborhood_hit'])\n",
    "print(score_mrre)\n",
    "print(score_srcc)\n",
    "print(score_spcc)\n",
    "print(score_dtm)\n",
    "print(score_catnc)\n",
    "print(score_cevmv)\n",
    "print(score_dsc)\n",
    "print(score_ivmv)\n",
    "print(score_kl)\n",
    "print(score_ltnc)\n",
    "print(score_lcmc)\n",
    "print(score_nd)\n",
    "print(score_nms)\n",
    "print(score_procrustes)\n",
    "print(score_stress)\n",
    "print(score_norm_stress)\n",
    "print(score_topographic)\n",
    "print(score_pps)\n",
    "print(score_pps_v2)\n",
    "print(score_ale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_heatmap_padded(ale_scores, title=\"Average Local Error Heatmap\"):\n",
    "    \"\"\"\n",
    "    Plot a heatmap of ALE scores, padding to a square grid if necessary.\n",
    "\n",
    "    Parameters:\n",
    "        ale_scores (numpy.ndarray): ALE scores for each point.\n",
    "        title (str): Title for the heatmap.\n",
    "    \"\"\"\n",
    "    n = len(ale_scores)\n",
    "    size = int(np.ceil(np.sqrt(n)))  # Find the smallest square grid\n",
    "    padded_scores = np.pad(ale_scores, (0, size**2 - n), constant_values=np.mean(ale_scores))\n",
    "    ale_grid = padded_scores.reshape((size, size))\n",
    "\n",
    "    # Create a heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(ale_grid, cmap=\"coolwarm\", annot=False, cbar=True)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"X-axis\")\n",
    "    plt.ylabel(\"Y-axis\")\n",
    "    plt.show()\n",
    "\n",
    "plot_heatmap_padded(score_ale, title=\"Average Local Error Heatmap\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = X.shape[0]  # Number of data points\n",
    "    \n",
    "# # Compute pairwise distances in the high-dimensional and low-dimensional spaces\n",
    "distances_O = pairwise_distances(X)  # High-dimensional distances\n",
    "distances_P = pairwise_distances(X_emb)  # Low-dimensional distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the indices of the n nearest neighbors in the high-dimensional space (excluding the point itself)\n",
    "# nearest_neighbors_indices = np.argsort(distances_O[i])[:n_neighbors+1][1:]\n",
    "nearest_neighbors_indices = np.argsort(distances_O[0])[:n_neighbors+1][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distance vectors for high-dimensional and low-dimensional spaces\n",
    "d_O = distances_O[0, nearest_neighbors_indices]\n",
    "d_P = distances_P[0, nearest_neighbors_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the distance vectors to unit length\n",
    "d_O_normalized = d_O / np.linalg.norm(d_O)\n",
    "d_P_normalized = d_P / np.linalg.norm(d_P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(d_O_normalized - d_P_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Compute pairwise distances in the original space (high-dimensional)\n",
    "dist_X = pairwise_distances(X, metric=metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.fill_diagonal(dist_X, np.inf)\n",
    "dist_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting to find nearest neighbors in the high-dimensional space\n",
    "ind_X = np.argsort(dist_X, axis=1)  #why axis 1?\n",
    "ind_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nearest neighbors in the low-dimensional embedded space\n",
    "neigh = NearestNeighbors(n_neighbors=n_neighbors)\n",
    "neigh.fit(X_emb)\n",
    "ind_X_embedded = neigh.kneighbors(return_distance=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 150\n",
    "# n_samples = _num_samples(X)\n",
    "inverted_index = np.zeros((n_samples, n_samples), dtype=int)\n",
    "inverted_index.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_indices = np.arange(n_samples + 1)\n",
    "ordered_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index[ordered_indices[:-1, np.newaxis], ind_X] = ordered_indices[1:]\n",
    "# inverted_index[ordered_indices[:-1, np.newaxis], ind_X_embedded] = ordered_indices[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_X_embedded[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_X[1][:10]  # below positive 3 represents outside the nearest neighbors in the high-dimensional space, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = (\n",
    "        inverted_index[ordered_indices[:-1, np.newaxis], ind_X_embedded] - n_neighbors\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = inverted_index[ordered_indices[:-1, np.newaxis], ind_X_embedded]\n",
    "temp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_continuity(high_dim_dt, low_dim_dt, n_neighbors=7):\n",
    "    \"\"\"\n",
    "    Calculate the Continuity metric for the given high-dimensional data\n",
    "    and its low-dimensional projection.\n",
    "    \"\"\"\n",
    "    n = high_dim_dt.shape[0]\n",
    "    \n",
    "    # Compute pairwise distances and find k-nearest neighbors in both spaces\n",
    "    dist_X = pairwise_distances(high_dim_dt)\n",
    "    dist_Y = pairwise_distances(low_dim_dt)\n",
    "    \n",
    "    # Get sorted indices based on distances\n",
    "    neighbors_X = np.argsort(dist_X, axis=1)[:, 1:n_neighbors+1]\n",
    "    neighbors_Y = np.argsort(dist_Y, axis=1)[:, 1:n_neighbors+1]\n",
    "    \n",
    "    # Calculate the continuity penalty\n",
    "    penalty = 0\n",
    "    for i in range(n):\n",
    "        missing_neighbors = set(neighbors_X[i]) - set(neighbors_Y[i])\n",
    "        # missing_neighbors = set(neighbors_Y[i]) - set(neighbors_X[i])\n",
    "        for j in missing_neighbors:\n",
    "            rank_in_Y = np.where(np.argsort(dist_Y[i]) == j)[0][0]\n",
    "            # rank_in_Y = np.where(np.argsort(dist_X[i]) == j)[0][0]\n",
    "            penalty += max(0, n_neighbors + 1 - rank_in_Y)\n",
    "    \n",
    "    # Normalize the penalty\n",
    "    normalization_factor = n * n_neighbors * (2 * n - 3 * n_neighbors - 1)\n",
    "    C = 1 - (2 / normalization_factor) * penalty\n",
    "    return C\n",
    "\n",
    "calculate_continuity(X,X_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_trustworthiness(high_dim_dt, low_dim_dt, n_neighbors=7):\n",
    "    \"\"\"\n",
    "    Calculate the Trustworthiness metric for the given high-dimensional data\n",
    "    and its low-dimensional projection.\n",
    "    \"\"\"\n",
    "    n = high_dim_dt.shape[0]\n",
    "    \n",
    "    # Compute pairwise distances and find k-nearest neighbors in both spaces\n",
    "    dist_X = pairwise_distances(high_dim_dt)\n",
    "    dist_Y = pairwise_distances(low_dim_dt)\n",
    "    \n",
    "    # Get sorted indices based on distances\n",
    "    neighbors_X = np.argsort(dist_X, axis=1)[:, 1:n_neighbors+1]\n",
    "    neighbors_Y = np.argsort(dist_Y, axis=1)[:, 1:n_neighbors+1]\n",
    "    \n",
    "    # Calculate the trustworthiness penalty\n",
    "    penalty = 0\n",
    "    for i in range(n):\n",
    "        # Check the neighbors that should be in Y but are not\n",
    "        missing_neighbors = set(neighbors_Y[i]) - set(neighbors_X[i])\n",
    "        for j in missing_neighbors:\n",
    "            rank_in_X = np.where(np.argsort(dist_X[i]) == j)[0][0]  # rank in the high-dimensional space\n",
    "            penalty += max(0, n_neighbors + 1 - rank_in_X)  # The trustworthiness penalty\n",
    "    \n",
    "    # Normalize the penalty\n",
    "    normalization_factor = n * n_neighbors * (2 * n - 3 * n_neighbors - 1)\n",
    "    T = 1 - (2 / normalization_factor) * penalty\n",
    "    return T\n",
    "\n",
    "# Example usage with X and X_emb:\n",
    "trustworthiness = calculate_trustworthiness(X, X_emb)\n",
    "trustworthiness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_j = ranks[ranks > 0]\n",
    "rank_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.sum(ranks[ranks > 0])\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jj = 1.0 - t * (\n",
    "        2.0 / (n_samples * n_neighbors * (2.0 * n_samples - 3.0 * n_neighbors - 1.0))\n",
    "    )\n",
    "jj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_X[0, 1:n_neighbors+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_X_embedded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(ind_X[0][:7]) - set(ind_X_embedded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example distance matrix (dist_X)\n",
    "dist_X = np.array([\n",
    "    [np.inf, 0.2, 0.5, 0.8],  # Distances from point 0 to others\n",
    "    [0.2, np.inf, 0.3, 0.7],  # Distances from point 1 to others\n",
    "    [0.5, 0.3, np.inf, 0.4],  # Distances from point 2 to others\n",
    "    [0.8, 0.7, 0.4, np.inf]   # Distances from point 3 to others\n",
    "])\n",
    "dist_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort indices by distance for each point\n",
    "ind_X = np.argsort(dist_X, axis=1)\n",
    "ind_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import gaussian_dt, iris_dt, digits_dt, covariance_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D, c = covariance_type()\n",
    "\n",
    "type(D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "len(np.unique(c))\n",
    "D.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris, load_wine, load_digits, fetch_openml\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Function to fetch UCI datasets via OpenML\n",
    "def fetch_uci_dataset(name, target_column):\n",
    "    data = fetch_openml(name=name, as_frame=True, parser=\"pandas\")\n",
    "    X = data.data\n",
    "    y = data.target\n",
    "    if target_column is not None:\n",
    "        y = X[target_column]\n",
    "        X = X.drop(columns=[target_column])\n",
    "    return X, y\n",
    "\n",
    "# Dictionary to dynamically load datasets\n",
    "dataset_loaders = {\n",
    "    \"iris\": lambda: load_iris(return_X_y=True),\n",
    "    \"wine\": lambda: load_wine(return_X_y=True),\n",
    "    \"digits\": lambda: load_digits(return_X_y=True),\n",
    "    \"human_activity\": lambda: fetch_uci_dataset(\"HumanActivityRecognitionUsingSmartphones\", None),\n",
    "    \"glass\": lambda: fetch_uci_dataset(\"glass\", \"Type\"),\n",
    "    \"breast_cancer\": lambda: load_iris(return_X_y=True),  # Replace with OpenML if needed\n",
    "    \"zoo\": lambda: fetch_uci_dataset(\"zoo\", \"type\"),\n",
    "    \"covertype\": lambda: fetch_uci_dataset(\"covertype\", \"Cover_Type\"),\n",
    "    \"newsgroups\": lambda: fetch_openml(data_id=110, as_frame=True, parser=\"pandas\"),  # Replace with text processing logic\n",
    "}\n",
    "\n",
    "# Storage for datasets\n",
    "datasets = {}\n",
    "\n",
    "# Load each dataset dynamically\n",
    "for name, loader in dataset_loaders.items():\n",
    "    print(f\"Loading dataset: {name}\")\n",
    "    try:\n",
    "        X, y = loader()\n",
    "        # Ensure labels are numeric for consistency\n",
    "        if isinstance(y, pd.Series):\n",
    "            y = LabelEncoder().fit_transform(y)\n",
    "        datasets[name] = {\"data\": np.array(X), \"labels\": np.array(y)}\n",
    "        print(f\"Dataset {name} loaded successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load dataset {name}: {e}\")\n",
    "\n",
    "# Access data and labels\n",
    "for dataset_name, dataset_content in datasets.items():\n",
    "    data = dataset_content[\"data\"]\n",
    "    labels = dataset_content[\"labels\"]\n",
    "    print(f\"Dataset: {dataset_name}, Data Shape: {data.shape}, Labels Shape: {labels.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "import pandas as pd\n",
    "\n",
    "# Load the Breast Cancer dataset\n",
    "data = load_breast_cancer()\n",
    "\n",
    "# Extract features and target labels\n",
    "X = data.data  # Feature data\n",
    "y = data.target  # Target labels\n",
    "\n",
    "# Convert to DataFrame for better readability\n",
    "breast_cancer_df = pd.DataFrame(X, columns=data.feature_names)\n",
    "breast_cancer_df['class'] = y\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", breast_cancer_df.shape)\n",
    "print(\"Classes:\", breast_cancer_df['class'].unique())\n",
    "print(\"First five rows:\")\n",
    "print(breast_cancer_df.head())\n",
    "\n",
    "# Check class distribution\n",
    "class_counts = breast_cancer_df['class'].value_counts()\n",
    "print(\"\\nClass distribution:\")\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def load_images_from_folder(folder_path, limit=30):\n",
    "    \"\"\"\n",
    "    Load PNG files from the specified folder, limited to the first 'limit' files.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing PNG files.\n",
    "    - limit (int): Maximum number of images to load.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: List of file paths to PNG images (up to the specified limit).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "    \n",
    "    image_files = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.lower().endswith(\".png\"):  # Check for PNG files\n",
    "            image_files.append(os.path.join(folder_path, file_name))\n",
    "    \n",
    "    # Sort the images and take only the first 'limit' images\n",
    "    return sorted(image_files)[:limit]\n",
    "\n",
    "\n",
    "def create_collage(image_files, output_file, images_per_row=4):\n",
    "    \"\"\"\n",
    "    Combine multiple PNG files into a single image with the original resolution.\n",
    "    \"\"\"\n",
    "    # Load images\n",
    "    images = [Image.open(file) for file in image_files]\n",
    "    \n",
    "    # Get the size of the images (assuming all have the same size)\n",
    "    image_width, image_height = images[0].size\n",
    "\n",
    "    # Calculate the dimensions of the collage\n",
    "    rows = (len(images) + images_per_row - 1) // images_per_row\n",
    "    cols = min(len(images), images_per_row)\n",
    "    collage_width = cols * image_width\n",
    "    collage_height = rows * image_height\n",
    "\n",
    "    # Create a blank canvas for the collage\n",
    "    collage = Image.new(\"RGB\", (collage_width, collage_height), \"white\")\n",
    "\n",
    "    # Paste images into the collage\n",
    "    for index, image in enumerate(images):\n",
    "        row, col = divmod(index, images_per_row)\n",
    "        x = col * image_width\n",
    "        y = row * image_height\n",
    "        collage.paste(image, (x, y))\n",
    "\n",
    "    # Save the collage\n",
    "    collage.save(output_file)\n",
    "    print(f\"Collage saved as {output_file}\")\n",
    "\n",
    "# Use the relative path\n",
    "dataset = 'tetrahedron_eq_2_close'\n",
    "# dataset = 'high_dim'\n",
    "method = 'tsne'\n",
    "num_dim = 200\n",
    "# output_folder = f\"thesis_reproduced/testing_new/new_final_results/{dataset}_{num_dim}/{method}_plots_new_model\"\n",
    "output_folder = f\"thesis_reproduced/testing_new/new_final_results/{dataset}/{method}_plots_new_model\"\n",
    "\n",
    "# Ensure the folder exists and load PNG files dynamically\n",
    "image_files = load_images_from_folder(output_folder)\n",
    "\n",
    "if not image_files:\n",
    "    print(\"No PNG files found in the specified folder!\")\n",
    "else:\n",
    "    output_file = f\"collage_{dataset}_{method}.png\"\n",
    "    create_collage(image_files, output_file, images_per_row=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def load_images_from_folder(folder_path, prefixes, limit=30):\n",
    "    \"\"\"\n",
    "    Load PNG files from the specified folder that start with any of the given prefixes.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing PNG files.\n",
    "    - prefixes (list of str): List of words that filenames should start with.\n",
    "    - limit (int): Maximum number of images to load.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: List of file paths to selected PNG images (up to the specified limit).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "    \n",
    "    selected_files = []\n",
    "    \n",
    "    for file_name in sorted(os.listdir(folder_path)):  # Sort for consistency\n",
    "        if file_name.lower().endswith(\".png\") and any(file_name.startswith(prefix) for prefix in prefixes):\n",
    "            selected_files.append(os.path.join(folder_path, file_name))\n",
    "        \n",
    "        if len(selected_files) >= limit:  # Stop when limit is reached\n",
    "            break\n",
    "    \n",
    "    return selected_files\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def create_collage(image_files, output_file, images_per_row=4, title=\"Collage Title\"):\n",
    "    \"\"\"\n",
    "    Combine multiple PNG files into a single image with uniform size and add a title.\n",
    "    \"\"\"\n",
    "    images = [Image.open(file) for file in image_files]\n",
    "    \n",
    "    # Find the smallest width and height among all images\n",
    "    min_width = min(img.width for img in images)\n",
    "    min_height = min(img.height for img in images)\n",
    "\n",
    "    # Resize images to the smallest found size\n",
    "    images = [img.resize((min_width, min_height)) for img in images]\n",
    "\n",
    "    # Compute collage dimensions\n",
    "    num_images = len(images)\n",
    "    cols = images_per_row\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "\n",
    "    collage_width = cols * min_width\n",
    "    collage_height = rows * min_height\n",
    "\n",
    "    # Add extra space for the title (e.g., 80 pixels height)\n",
    "    title_height = 80\n",
    "    total_height = collage_height + title_height\n",
    "\n",
    "    # Create a blank canvas with extra space for title\n",
    "    collage = Image.new(\"RGB\", (collage_width, total_height), \"white\")\n",
    "\n",
    "    # Add title\n",
    "    draw = ImageDraw.Draw(collage)\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"arial.ttf\", 80)  # Load a font (adjust size)\n",
    "    except IOError:\n",
    "        font = ImageFont.load_default()  # Use default font if not found\n",
    "\n",
    "    # Get text size using textbbox() instead of textsize()\n",
    "    text_bbox = draw.textbbox((0, 0), title, font=font)\n",
    "    text_width = text_bbox[2] - text_bbox[0]\n",
    "    text_height = text_bbox[3] - text_bbox[1]\n",
    "\n",
    "    # Center the title\n",
    "    text_x = (collage_width - text_width) // 2  \n",
    "    text_y = 20  # Padding from the top\n",
    "    draw.text((text_x, text_y), title, fill=\"black\", font=font)\n",
    "\n",
    "    # Paste images below the title\n",
    "    for index, image in enumerate(images):\n",
    "        row, col = divmod(index, cols)\n",
    "        x = col * min_width\n",
    "        y = title_height + row * min_height  # Offset by title height\n",
    "        collage.paste(image, (x, y))\n",
    "\n",
    "    # Save the final collage with high quality\n",
    "    collage.save(output_file, format=\"PNG\", dpi=(300, 300))\n",
    "\n",
    "    print(f\"✅ Collage saved as {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "# def create_collage(image_files, output_file, images_per_row=4):\n",
    "#     \"\"\"\n",
    "#     Combine multiple PNG files into a single image with uniform size.\n",
    "#     \"\"\"\n",
    "#     # Load images and resize to the smallest found size (to avoid overlapping)\n",
    "#     images = [Image.open(file) for file in image_files]\n",
    "    \n",
    "#     # Find the smallest width and height among all images (to maintain uniformity)\n",
    "#     min_width = min(img.width for img in images)\n",
    "#     min_height = min(img.height for img in images)\n",
    "    \n",
    "#     # Resize all images to the smallest found size\n",
    "#     images = [img.resize((min_width, min_height)) for img in images]\n",
    "\n",
    "#     # Calculate the collage dimensions\n",
    "#     num_images = len(images)\n",
    "#     cols = images_per_row  # Fixed number of columns\n",
    "#     rows = (num_images + cols - 1) // cols  # Compute necessary rows\n",
    "\n",
    "#     collage_width = cols * min_width\n",
    "#     collage_height = rows * min_height\n",
    "\n",
    "#     # Create a blank white canvas\n",
    "#     collage = Image.new(\"RGB\", (collage_width, collage_height), \"white\")\n",
    "\n",
    "#     # Paste images into the collage\n",
    "#     for index, image in enumerate(images):\n",
    "#         row, col = divmod(index, cols)\n",
    "#         x = col * min_width\n",
    "#         y = row * min_height\n",
    "#         collage.paste(image, (x, y))\n",
    "\n",
    "#     # Save the final collage\n",
    "#     # collage.save(output_file, quality=95)\n",
    "#     collage.save(output_file, format=\"PNG\", dpi=(300, 300))\n",
    "\n",
    "#     print(f\"✅ Collage saved as {output_file}\")\n",
    "\n",
    "\n",
    "# Use the relative path\n",
    "dataset = 'tetrahedron_eq_2_close'\n",
    "method = 'tsne'\n",
    "num_dim = 200\n",
    "output_folder = f\"thesis_reproduced/testing_new/new_final_results/{dataset}/{method}_plots_new_model\"\n",
    "\n",
    "prefix_list = [\"HD_clust_distance\", \"spectral_norm\", \"barycentric_interpolation\",\n",
    "               'ftle_interpolate', 'delaunay_tri_MAX', 'Fully_connected']  # List of words to filter by\n",
    "# Load PNG files dynamically\n",
    "image_files = load_images_from_folder(output_folder, prefix_list)\n",
    "\n",
    "if not image_files:\n",
    "    print(\"❌ No PNG files found in the specified folder!\")\n",
    "else:\n",
    "    output_file = f\"collage_{dataset}_{method}.png\"\n",
    "    create_collage(image_files, output_file, images_per_row=4, title= dataset)  # Set number of columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Collage saved as collage_digits_tsne.png\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import os\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def load_images_from_folder(folder_path, prefixes, limit=30):\n",
    "    \"\"\"\n",
    "    Load PNG files from the specified folder that start with any of the given prefixes.\n",
    "    \n",
    "    Parameters:\n",
    "    - folder_path (str): Path to the folder containing PNG files.\n",
    "    - prefixes (list of str): List of words that filenames should start with.\n",
    "    - limit (int): Maximum number of images to load.\n",
    "\n",
    "    Returns:\n",
    "    - list of str: List of file paths to selected PNG images (up to the specified limit).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder_path):\n",
    "        raise FileNotFoundError(f\"Folder not found: {folder_path}\")\n",
    "    \n",
    "    selected_files = []\n",
    "    \n",
    "    for file_name in sorted(os.listdir(folder_path)):  # Sort for consistency\n",
    "        if file_name.lower().endswith(\".png\") and any(file_name.startswith(prefix) for prefix in prefixes):\n",
    "            selected_files.append(os.path.join(folder_path, file_name))\n",
    "        \n",
    "        if len(selected_files) >= limit:  # Stop when limit is reached\n",
    "            break\n",
    "    \n",
    "    return selected_files\n",
    "\n",
    "def create_collage(image_files, output_file, images_per_row=4, title=\"Collage Title\"):\n",
    "    \"\"\"\n",
    "    Create a collage from images and add a title and individual figure labels.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_files (list): List of image file paths.\n",
    "    - output_file (str): Path to save the final collage.\n",
    "    - images_per_row (int): Number of images per row.\n",
    "    - title (str): Title for the collage.\n",
    "    \"\"\"\n",
    "    images = [Image.open(file) for file in image_files]\n",
    "    \n",
    "    # Find the smallest width and height among all images\n",
    "    min_width = min(img.width for img in images)\n",
    "    min_height = min(img.height for img in images)\n",
    "\n",
    "    # Resize images to uniform size\n",
    "    images = [img.resize((min_width, min_height)) for img in images]\n",
    "\n",
    "    # Compute collage dimensions\n",
    "    num_images = len(images)\n",
    "    cols = images_per_row\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "\n",
    "    collage_width = cols * min_width\n",
    "    title_height = 100  # Space for the main title\n",
    "    figure_label_height = 40  # Space for each figure label\n",
    "    collage_height = rows * (min_height + figure_label_height)  # Add space for labels\n",
    "    total_height = collage_height + title_height  # Total height including main title\n",
    "\n",
    "    # Create a blank canvas with space for title and labels\n",
    "    collage = Image.new(\"RGB\", (collage_width, total_height), \"white\")\n",
    "\n",
    "    # Draw title\n",
    "    draw = ImageDraw.Draw(collage)\n",
    "    try:\n",
    "        font_title = ImageFont.truetype(\"arial.ttf\", 40)  # Title font\n",
    "        font_label = ImageFont.truetype(\"arial.ttf\", 30)  # Label font\n",
    "    except IOError:\n",
    "        font_title = ImageFont.load_default()\n",
    "        font_label = ImageFont.load_default()\n",
    "\n",
    "    # Get text size for the main title\n",
    "    text_bbox = draw.textbbox((0, 0), title, font=font_title)\n",
    "    text_width = text_bbox[2] - text_bbox[0]\n",
    "    text_x = (collage_width - text_width) // 2\n",
    "    text_y = 20  # Title at the top\n",
    "    draw.text((text_x, text_y), title, fill=\"black\", font=font_title)\n",
    "\n",
    "    # Paste images and add labels\n",
    "    for index, (image, file_path) in enumerate(zip(images, image_files)):\n",
    "        row, col = divmod(index, cols)\n",
    "        x = col * min_width\n",
    "        y = title_height + row * (min_height + figure_label_height)  # Adjust for label\n",
    "\n",
    "        collage.paste(image, (x, y))\n",
    "\n",
    "        # Extract filename (without extension) for the label\n",
    "        label = os.path.basename(file_path).split('.')[0]\n",
    "        label = label[:20] + \"...\" if len(label) > 23 else label  # Trim long names\n",
    "\n",
    "        # Get label text size and center it\n",
    "        label_bbox = draw.textbbox((0, 0), label, font=font_label)\n",
    "        label_width = label_bbox[2] - label_bbox[0]\n",
    "        label_x = x + (min_width - label_width) // 2\n",
    "        label_y = y + min_height + 5  # Position below image\n",
    "        draw.text((label_x, label_y), label, fill=\"black\", font=font_label)\n",
    "\n",
    "    # Save the final collage\n",
    "    collage.save(output_file, format=\"PNG\", dpi=(300, 300))\n",
    "    print(f\"✅ Collage saved as {output_file}\")\n",
    "\n",
    "# # Example Usage\n",
    "# output_file = f\"collage_{dataset}_{method}.png\"\n",
    "# create_collage(image_files, output_file, images_per_row=4, title=dataset)\n",
    "\n",
    "\n",
    "# Use the relative path\n",
    "# dataset = 'high_dim'\n",
    "dataset = 'digits'\n",
    "method = 'tsne'\n",
    "num_dim = 500\n",
    "# if dataset == high_dim:\n",
    "\n",
    "#     output_folder = f\"thesis_reproduced/testing_new/new_final_results/{dataset}_{high_dim}/{method}_plots_new_model\"\n",
    "# else:\n",
    "#     output_folder = f\"thesis_reproduced/testing_new/new_final_results/{dataset}/{method}_plots_new_model\"\n",
    "\n",
    "###_____________ Output folder__________________________________\n",
    "if dataset == 'high_dim':\n",
    "    output_folder = f\"thesis_reproduced/testing_new/new_final_results/{dataset}_{num_dim}/{method}_plots_new_model\"\n",
    "    dataset = f'{dataset}_{num_dim}'\n",
    "else:\n",
    "    output_folder = f\"thesis_reproduced/testing_new/new_final_results/{dataset}/{method}_plots_new_model\"\n",
    "\n",
    "##_____________________________________________________________________________\n",
    "\n",
    "prefix_list = [\"HD_clust_distance\", \"spectral_norm_t-SNE\", \"barycentric\",\n",
    "                'Delanay_with_selected_sub','Fully_sub_connected']  # List of words to filter by\n",
    "\n",
    "\n",
    "\n",
    "# Load PNG files dynamically\n",
    "image_files = load_images_from_folder(output_folder, prefix_list)\n",
    "\n",
    "if not image_files:\n",
    "    print(\"❌ No PNG files found in the specified folder!\")\n",
    "else:\n",
    "    output_file = f\"collage_{dataset}_{method}.png\"\n",
    "    create_collage(image_files, output_file, images_per_row=4, title= dataset)  # Set number of columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'gaussian'\n",
    "method = 'tsne'\n",
    "prj_metric_path_1 = f\"thesis_reproduced/testing_new/final_results/{dataset}/{method}_plots_old_model/hd_ld_metrics/{dataset}_{method}_prj_metrics_hd_ld.pkl\"\n",
    "prj_metric_path_2 = f\"thesis_reproduced/testing_new/final_results/Upload_documents_final_results/{dataset}/{method}_plots_old_model/hd_ld_metrics/{dataset}_{method}_prj_metrics_hd_ld.pkl\"\n",
    "jacob_path_1 = f\"thesis_reproduced/testing_new/final_results/{dataset}/{method}_plots_old_model/jacobian_norms/{dataset}_{method}_jacobian_norm.pkl\"\n",
    "jacob_path_2 = f\"thesis_reproduced/testing_new/final_results/Upload_documents_final_results/{dataset}/{method}_plots_old_model/jacobian_norms/{dataset}_{method}_jacobian_norm.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_metric_path_2\n",
    "E:\\inverse_projection_visual_analytics\\thesis_reproduced\\testing_new\\final_results\\Upload_documents_final_results\\gaussian\\tsne_plots_old_model\\hd_ld_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import *\n",
    "\n",
    "prj_metric_1 = load_metrics(prj_metric_path_1)\n",
    "jacob_norm_1 = load_metrics(jacob_path_1)\n",
    "prj_metric_2 = load_metrics(prj_metric_path_2)\n",
    "jacob_norm_2 = load_metrics(jacob_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jacob_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prj_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dist= np.array([[0.        , 0.0612345 , 0.06683991, 0.08736833, 0.0391083 ],\n",
    "       [0.0612345 , 0.        , 0.0586919 , 0.04931701, 0.07375501],\n",
    "       [0.06683991, 0.0586919 , 0.        , 0.03876263, 0.05326245],\n",
    "       [0.08736833, 0.04931701, 0.03876263, 0.        , 0.08626064],\n",
    "       [0.0391083 , 0.07375501, 0.05326245, 0.08626064, 0.        ]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = [[0.        , 0.0612345 , 0.06683991, 0.08736833, 0.0391083 ,\n",
    "        1.06945709, 1.05681305, 1.07188269, 1.03027446],\n",
    "       [0.0612345 , 0.        , 0.0586919 , 0.04931701, 0.07375501,\n",
    "        1.0862432 , 1.07366997, 1.08710053, 1.04968977],\n",
    "       [0.06683991, 0.0586919 , 0.        , 0.03876263, 0.05326245,\n",
    "        1.07375474, 1.06170735, 1.07692937, 1.03563016],\n",
    "       [0.08736833, 0.04931701, 0.03876263, 0.        , 0.08626064,\n",
    "        1.06964567, 1.05753809, 1.07147128, 1.03337137],\n",
    "       [0.0391083 , 0.07375501, 0.05326245, 0.08626064, 0.        ,\n",
    "        1.0895343 , 1.07721707, 1.09320604, 1.04960552]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inverse-projection-j9xKauoB-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
